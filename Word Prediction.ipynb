{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh9LDG_BUuCo"
      },
      "source": [
        "# INSTRUCTIONS TO RUN THE NOTEBOOK\n",
        "\n",
        "1. Load the required libraries.\n",
        "2. Load the pre-fitted tokenizer and tokenized sequence.\n",
        "3. (3.1) In case you want to train the model from scratch, you can either explore the best hyperparameter values by running the code in the 'Hyperparameter tuning on a subset of data' section, or directly skip to the 'Training' section.\n",
        "   (3.2) If you want to use the pretrained model, run the code in the 'Generate text' section, which loads the model and defines the function for text generation. This function accepts as an input both a string of any length of words, as well as a sequence of token indices. The second argument of the function: 'seq_len' must be set to 20, but the input text and the desired number of output words can be modified as the user wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qzu03UrHVSU8",
        "outputId": "06609396-c44f-4677-e4d1-266e602534c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T13:08:42.413789Z",
          "start_time": "2023-02-15T13:08:32.302414Z"
        },
        "id": "RZz0xoQ9VsnJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump, load\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np \n",
        "import random as rn\n",
        "from keras import backend as K\n",
        "import string\n",
        "# from scikeras.wrappers import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T13:08:54.021940Z",
          "start_time": "2023-02-15T13:08:52.589372Z"
        },
        "id": "-enkeUdhRdK2"
      },
      "outputs": [],
      "source": [
        "# set seeds for reproducibility\n",
        "seed_value = 0\n",
        "np.random.seed(seed_value)\n",
        "rn.seed(seed_value)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(seed_value)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T11:29:47.227787Z",
          "start_time": "2023-02-15T11:28:50.104123Z"
        },
        "id": "4jYZKcWrbJuy"
      },
      "outputs": [],
      "source": [
        "# this function eliminates non-alphanumeric tokens and lower-cases all words\n",
        "def clean(doc):\n",
        "    tokens = doc.split()\n",
        "    table = str.maketrans(\"\",\"\",string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Y1zrk6Wjx8"
      },
      "source": [
        "# Load fitted tokenizer and tokenized sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T13:10:48.970225Z",
          "start_time": "2023-02-15T13:10:25.434702Z"
        },
        "id": "sJppdHqGWjx8"
      },
      "outputs": [],
      "source": [
        "# load the tokenizer that was fitted on the entire inital data sequence\n",
        "tokenizer = load(open('drive/MyDrive/Language_data/tokenizer.pkl', 'rb'))\n",
        "# Load the tokenized sequence\n",
        "sequences = load(open('drive/MyDrive/Language_data/sequence_full_tokenized.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T13:11:57.281210Z",
          "start_time": "2023-02-15T13:11:08.896796Z"
        },
        "id": "SSvX_aXyWjx8"
      },
      "outputs": [],
      "source": [
        "seq_good = list(np.concatenate(sequences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq5_rUvhWjx9"
      },
      "source": [
        "# Hyperparameter tuning on a subset of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-02-15T14:48:09.771800Z",
          "start_time": "2023-02-15T14:45:52.062505Z"
        },
        "id": "felm-jWhWjyB"
      },
      "outputs": [],
      "source": [
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "\n",
        "#define hyperparameter search space for tuning\n",
        "space = {'seq_len': hp.choice('seq_len', [10,20,50]), \n",
        "         'embedding': hp.choice('embedding', [500,1000]),\n",
        "         'units1': hp.choice('units1', [64,128]),\n",
        "         'units2': hp.choice('units2', [64,128]),\n",
        "         'batch_size' : hp.choice('batch_size', [128,256]),\n",
        "         'nb_epochs' : 1,\n",
        "         'optimizer': hp.choice('optimizer',['adam']),\n",
        "         'activation': 'relu'}\n",
        "\n",
        "#We only use 10% of the data for this process, and divide that subset into a training and validation set. \n",
        "#The optimization is done by studying the accuracy in the validation set.\n",
        "train, val = seq_good[0:int(np.round(len(seq_good)*0.1*0.8))], seq_good[int(np.round(len(seq_good)*0.1*0.8)): int(np.round(len(seq_good)*0.1))]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "def create_model(params):\n",
        "    #create model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, params['embedding'], input_length=params['seq_len']))\n",
        "    model.add(LSTM(params['units1'], return_sequences=True))\n",
        "    model.add(LSTM(params['units2']))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    return model\n",
        "    \n",
        "def f_nn(params):   \n",
        "    print ('Params testing: ', params)\n",
        "\n",
        "    # We need to include this in the function to be optimised to select the best input sequence length\n",
        "    train_data_gen = TimeseriesGenerator(train, train, length=params['seq_len'], sampling_rate=1,stride=1, batch_size=params['batch_size'])\n",
        "    val_data_gen = TimeseriesGenerator(val, val, length=params['seq_len'], sampling_rate=1,stride=1, batch_size=params['batch_size'])\n",
        "    \n",
        "    model = create_model(params)\n",
        "    es = EarlyStopping(monitor='accuracy', patience=10)\n",
        "    callbacks_list = [es]\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_data_gen, epochs=params['nb_epochs'],callbacks=callbacks_list)\n",
        "\n",
        "    y_true = val_data_gen.labels\n",
        "    yhat = model.predict(val_data_gen)\n",
        "    y_pred = [np.argmax(i) for i in yhat]\n",
        "    \n",
        "    correct = (y_true == y_pred)\n",
        "    accuracy = correct.sum() / correct.size\n",
        "    return {'acc': -accuracy, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(f_nn, space, algo=tpe.suggest, max_evals=5, trials=trials)\n",
        "print('best: ', best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQQnxrTGOXpO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yg40Jp38K3pQ"
      },
      "outputs": [],
      "source": [
        "# Params {'activation': 'relu', 'batch_size': 64, 'embedding': 100, 'nb_epochs': 100, 'optimizer': 'adam', 'seq_len': 20, 'units1': 64, 'units2': 128}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuTcWKPHNMtT"
      },
      "outputs": [],
      "source": [
        "# Divide data intro training and validation\n",
        "train, val = seq_good[0:int(np.round(len(seq_good)*0.8))], seq_good[int(np.round(len(seq_good)*0.8)):]\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rixM02ETCW9J"
      },
      "outputs": [],
      "source": [
        "# create data generator\n",
        "train_data_gen = TimeseriesGenerator(train, train, length=20, sampling_rate=1,stride=20, batch_size=64)\n",
        "val_data_gen = TimeseriesGenerator(val, val, length=20, sampling_rate=1,stride=20, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZagvjusA23e"
      },
      "outputs": [],
      "source": [
        "# # Define model architecture accordingly to the results of the previous section\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=20))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgRfNz0tCqLF"
      },
      "outputs": [],
      "source": [
        "sav = tf.keras.callbacks.ModelCheckpoint('drive/MyDrive/Language_data/models',monitor='val_accuracy',save_freq='epoch',save_best_only=True)\n",
        "es = EarlyStopping(monitor='accuracy', patience=10)\n",
        "callbacks_list = [es, sav]\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lM1IgzmshSXV"
      },
      "outputs": [],
      "source": [
        "train model\n",
        "model.fit(train_data_gen, epochs=100, validation_data = val_data_gen, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmsTU-4vUuC2"
      },
      "source": [
        "# Continue training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6wFlkDiUuC2"
      },
      "outputs": [],
      "source": [
        "# load pretrained models\n",
        "model = tf.keras.models.load_model('drive/MyDrive/Language_data/models') cambiar a models_new si hacemos mas "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sav = tf.keras.callbacks.ModelCheckpoint('drive/MyDrive/Language_data/models_new',monitor='val_accuracy',save_freq='epoch',save_best_only=True)\n",
        "es = EarlyStopping(monitor='accuracy', patience=10)\n",
        "callbacks_list = [es, sav]\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qD78n8LZajzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data_gen, epochs=100, validation_data = val_data_gen, callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "t3movenoalMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHRWe8A9Dnnt"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pretrained models\n",
        "model = tf.keras.models.load_model('drive/MyDrive/Language_data/models')"
      ],
      "metadata": {
        "id": "Q1ZCo0IWhDM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc2croQaEACD"
      },
      "outputs": [],
      "source": [
        "seed_text = np.array([seq_good[100:110]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv7HvAVGO66E"
      },
      "outputs": [],
      "source": [
        "def translate_wordindices_to_words(seed_text):\n",
        "  # this function translates a list of word indices to actual words  \n",
        "  seed_translated = []\n",
        "\n",
        "  for seed in seed_text[0]:\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == seed:\n",
        "          out_word = word\n",
        "          break\n",
        "    seed_translated.append(out_word)\n",
        "\n",
        "  return seed_translated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldxlWa54O6zg"
      },
      "outputs": [],
      "source": [
        "def translate_words_to_wordindices(textaco):\n",
        "  # generate sequence of indices to be inputed to the model, following the same pre-processing process as earlier\n",
        "  teta_c = clean(textaco)\n",
        "  teta_t = tokenizer.texts_to_sequences(teta_c)\n",
        "  s = np.array([[ele for sublist in teta_t for ele in sublist]])\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTEuNMGdPi8b"
      },
      "outputs": [],
      "source": [
        "def next_word_predictor(model, seq_len, n_words, seed_thing, typo):\n",
        "  # This function uses the pre-trained model to generate a user defined number of words to continue a user defined input sequence\n",
        "  if typo == 'text': # if the input is a phrase or a string of several words, it is transformed into a sequence of numbers to be inputted to the model\n",
        "    save_thing = seed_thing\n",
        "    seed_thing = translate_words_to_wordindices(seed_thing)\n",
        "  else:\n",
        "    seed_thing = seed_thing\n",
        "  \n",
        "  generated_words = []\n",
        "  generated_indices = []\n",
        "\n",
        "  if len(seed_text[0]) < seq_len: # if the input sequence length is smaller than required, the sequence is padded\n",
        "    seed_thing = pad_sequences(seed_thing, maxlen=seq_len, truncating='pre')\n",
        "\n",
        "  for i in range(n_words): # we are going to predict n_words new words\n",
        "    \n",
        "    if len(generated_indices) < seq_len: \n",
        "    \n",
        "      if len(generated_indices) > 0: # if we have generated less words than the input sequence length, the input sequence for the next iteration must contain the new generated words\n",
        "                                     \n",
        "        for i in range(len(generated_indices)):\n",
        "          i = i+1\n",
        "          seed_thing[0][-i] = generated_indices[len(generated_indices)-i]\n",
        "\n",
        "      if len(generated_indices) == 0: # if no words have yet been generated, the input is not modified\n",
        "        seed_thing=seed_thing\n",
        "    \n",
        "    if len(generated_indices) >= seq_len: # if we generate more words than the sequence length, the input sequence to predict the next word will be the 20 previously predicted ones\n",
        "      new_generated = generated_indices[-seq_len:]\n",
        "      for i in range(len(new_generated)):\n",
        "        i = i+1\n",
        "        seed_thing[0][-i] = new_generated[len(new_generated)-i]\n",
        "    \n",
        "    \n",
        "    ypred = model.predict(seed_thing, verbose=0) # predict next word using pretrained model\n",
        "    yhat = np.array([np.argmax(ypred)])\n",
        "    generated_indices.append(yhat)\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "    generated_words.append(out_word)\n",
        "  \n",
        "  # Generate final sequence\n",
        "  final = ''\n",
        "  for i in generated_words:\n",
        "    final = final + ' ' + i\n",
        "\n",
        "  if typo == 'text':\n",
        "    print(save_thing + final)\n",
        "  if typo == 'array':\n",
        "    trans_input = translate_wordindices_to_words(seed_thing)\n",
        "    pre = ''\n",
        "    for i in trans_input:\n",
        "      pre = pre + ' ' + i\n",
        "    print(pre+final)\n",
        "    \n",
        "  return generated_words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gentxt = next_word_predictor(model, 20, 6, 'The two young men thought that they could survive', 'text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t31Sbf8o3aCU",
        "outputId": "5a13b82d-38b9-42d7-8964-f66e64a250a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The two young men thought that they could survive somewhere unlikely entirely to exercise numbers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gentxt = next_word_predictor(model, 20, 5, 'They were having a fun time until the two young men had to', 'text')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGF3imjx9wpW",
        "outputId": "a2f46f6d-e87a-4a6e-b550-4f5c6dbd9c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They were having a fun time until the two young men had to pass directly above display eleven\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}